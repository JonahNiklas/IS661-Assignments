{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Analytics I HWS 23/24\n",
    "\n",
    "# Home Assignment 4 (38pts)\n",
    "\n",
    "**There is a shortcut to get the 17pts from Task 2 without implementing Task 1, see Task 2e)**\n",
    "\n",
    "\n",
    "Submit your solution via Ilias until 23.59h on Friday, November 24th. Late submissions are **not possible**.\n",
    "\n",
    "Submit your solutions in teams of 3-4 students. Unless explicitly agreed otherwise in advance, **submissions from teams with more or less members will NOT be graded**.\n",
    "List all members of the team with their student ID in the cell below, and submit only one notebook per team. Only submit a notebook, do not submit the dataset(s) you used. Also, do NOT compress/zip your submission!\n",
    "\n",
    "You may use the code from the exercises and basic functionalities that are explained in official documentation of Python packages without citing, __all other sources must be cited__. In case of plagiarism (copying solutions from other teams or from the internet) ALL team members may be expelled from the course without warning.\n",
    "\n",
    "#### General guidelines:\n",
    "* Make sure that your code is executable, any task for which the code does not directly run on our machine will be graded with 0 points.\n",
    "* If you use packages that are not available on the default or conda-forge channel, list them below. Also add a link to installation instructions. \n",
    "* Ensure that the notebook does not rely on the current notebook or system state!\n",
    "  * Use `Kernel --> Restart & Run All` to see if you are using any definitions, variables etc. that \n",
    "    are not in scope anymore.\n",
    "  * Do not rename any of the datasets you use, and load it from the same directory that your ipynb-notebook is located in, i.e., your working directory.\n",
    "* Make sure you clean up your code before submission, e.g., properly align your code, and delete every line of code that you do not need anymore, even if you may have experimented with it. Minimize usage of global variables. Avoid reusing variable names multiple times!\n",
    "* Ensure your code/notebook terminates in reasonable time.\n",
    "* Feel free to use comments in the code. While we do not require them to get full marks, they may help us in case your code has minor errors.\n",
    "* For questions that require a textual answer, please do not write the answer as a comment in a code cell, but in a Markdown cell below the code. Always remember to provide sufficient justification for all answers.\n",
    "* You may create as many additional cells as you want, just make sure that the solutions to the individual tasks can be found near the corresponding assignment.\n",
    "* If you have any general question regarding the understanding of some task, do not hesitate to post in the student forum in Ilias, so we can clear up such questions for all students in the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# studentIDs of all team members\n",
    "team_members = [12345,67899,880800,234242]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional packages (if any):\n",
    " - Example: `powerlaw`, https://github.com/jeffalstott/powerlaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union, Dict, Set, Tuple\n",
    "from numpy.typing import NDArray\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Term Frequency - Inverse Document Frequency (21 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task we want to use the term frequency - inverse document frequency (tf-idf) weighting method to compare documents with each other and to queries. In the end, we will apply our method to a subset of wikipedia pages (more specifically: only the introduction sections) that are linked to from the English Wikipedia page of Mannheim.\n",
    "\n",
    "In case you need to tokenize any sentences in the following tasks, please use a tokenizer from NLTK and not the ``string.split`` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__a)__ To test your implementation throughout this task, you are given the example from exercise 8. Start by implementing a function ``process_docs`` that takes the provided dictionary of documents and returns the following data structures. __(4 pts)__\n",
    "\n",
    "- ``word2index``: a dictionary that maps each word that appears in any document to a unique integer identifier starting at 0 \n",
    "- ``doc2index``: a dictionary that maps each document name (here given as the dictionary keys) to a unique integer identifier starting at 0\n",
    "- ``index2doc``: a dictionary that maps each document identifier to the corresponding document name (reverse to ``doc2index``)\n",
    "- ``doc_word_vectors``: a dictionary that maps each document name to a list of word ids that indicate which words appeared in the document in their order of appearance. Words that appear multiple times must also be included multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example from exercise 8\n",
    "d1 = \"cold beer beach\"\n",
    "d2 = \"ice cream beer beer\"\n",
    "d3 = \"beach cold ice cream\"\n",
    "d4 = \"cold beer frozen yogurt frozen beer\"\n",
    "d5 = \"frozen ice ice beer ice cream\"\n",
    "d6 = \"yogurt ice cream ice cream\"\n",
    "\n",
    "docs = {\"d1\": d1, \"d2\": d2, \"d3\": d3, \"d4\": d4, \"d5\": d5, \"d6\": d6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2index: {'cold': 0, 'beer': 1, 'beach': 2, 'ice': 3, 'cream': 4, 'frozen': 5, 'yogurt': 6}\n",
      "doc2index: {'d1': 0, 'd2': 1, 'd3': 2, 'd4': 3, 'd5': 4, 'd6': 5}\n",
      "index2doc: {0: 'd1', 1: 'd2', 2: 'd3', 3: 'd4', 4: 'd5', 5: 'd6'}\n",
      "doc_word_vectors: {'d1': [0, 1, 2], 'd2': [3, 4, 1, 1], 'd3': [2, 0, 3, 4], 'd4': [0, 1, 5, 6, 5, 1], 'd5': [5, 3, 3, 1, 3, 4], 'd6': [6, 3, 4, 3, 4]}\n"
     ]
    }
   ],
   "source": [
    "def process_docs(docs: Dict[str, str]) -> (Dict[str, int], Dict[str, int], Dict[int, str], Dict[str, List[int]]):\n",
    "    \"\"\"\n",
    "    :params docs: dict that maps each document name to the document content\n",
    "    :returns:\n",
    "        - word2index: dict that maps each word to a unique id\n",
    "        - doc2index: dict that maps each document name to a unique id\n",
    "        - index2doc: dict that maps ids to their associated document name\n",
    "        - doc_word_vectors: dict that maps each document name to a list of word ids that appear in it\n",
    "    \"\"\"\n",
    "    word2index = {}\n",
    "    doc2index = {}\n",
    "    index2doc = {}\n",
    "    doc_word_vectors = {}\n",
    "\n",
    "    word_index = 0\n",
    "    doc_index = 0\n",
    "\n",
    "    for doc_name, doc_content in docs.items():\n",
    "        # Tokenize the document content using NLTK tokenizer\n",
    "        tokens = word_tokenize(doc_content.lower())  # Convert to lowercase for consistency\n",
    "\n",
    "        doc_word_vectors[doc_name] = []\n",
    "\n",
    "        for token in tokens:\n",
    "            if token not in word2index:\n",
    "                word2index[token] = word_index\n",
    "                word_index += 1\n",
    "\n",
    "            doc_word_vectors[doc_name].append(word2index[token])\n",
    "\n",
    "        doc2index[doc_name] = doc_index\n",
    "        index2doc[doc_index] = doc_name\n",
    "        doc_index += 1\n",
    "\n",
    "    return word2index, doc2index, index2doc, doc_word_vectors\n",
    "\n",
    "word2index, doc2index, index2doc, doc_word_vectors = process_docs(docs)\n",
    "\n",
    "# Print the results\n",
    "print(\"word2index:\", word2index)\n",
    "print(\"doc2index:\", doc2index)\n",
    "print(\"index2doc:\", index2doc)\n",
    "print(\"doc_word_vectors:\", doc_word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output for the provided example could look like this:\n",
    "\n",
    "# word2index:\n",
    "# {'cold': 0, 'beer': 1, 'beach': 2, 'ice': 3, 'cream': 4, 'frozen': 5, 'yogurt': 6}\n",
    "\n",
    "# doc2index:\n",
    "# {'d1': 0, 'd2': 1, 'd3': 2, 'd4': 3, 'd5': 4, 'd6': 5}\n",
    "\n",
    "# index2doc\n",
    "# {0: 'd1', 1: 'd2', 2: 'd3', 3: 'd4', 4: 'd5', 5: 'd6'}\n",
    "\n",
    "# doc_word_vectors:\n",
    "# {'d1': [0, 1, 2],\n",
    "#  'd2': [3, 4, 1, 1],\n",
    "#  'd3': [2, 0, 3, 4],\n",
    "#  'd4': [0, 1, 5, 6, 5, 1],\n",
    "#  'd5': [5, 3, 3, 1, 3, 4],\n",
    "#  'd6': [6, 3, 4, 3, 4]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__b)__ Set up a term-document matrix where each column corresponds to a document and each row corresponds to a word that was observed in any of the documents. The row/column indices should correspond to the word/document ids that are set in the input dicts ``word2index`` and ``doc2index``. Count how often each word appears in each document and fill the term document matrix. __(3 pts)__\n",
    "\n",
    "_Example: The word \"beer\" with the word id_ ``1`` _appears two times in the document \"d4\" that has the document id_ ``3``. _Therefore the the entry at position_ ``[1, 3]`` _in the term-document matrix is_ ``2``.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 1. 1. 0. 0.]\n",
      " [1. 2. 0. 2. 1. 0.]\n",
      " [1. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 1. 0. 3. 2.]\n",
      " [0. 1. 1. 0. 1. 2.]\n",
      " [0. 0. 0. 2. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "def term_document_matrix(doc_word_v: Dict[str, List[int]], doc2index: Dict[str, int], word2index: Dict[str, int]) -> NDArray[NDArray[float]]:\n",
    "    \"\"\"\n",
    "    :param doc_word_v: dict that maps each document to the list of word ids that appear in it\n",
    "    :param doc2index: dict that maps each document name to a unique id\n",
    "    :param word2index: dict that maps each word to a unique id\n",
    "    :return: term-document matrix (each word is a row, each document is a column) that indicates the count of each word in each document \n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    num_words = len(word2index)\n",
    "    num_docs = len(doc2index)\n",
    "\n",
    "    term_doc_matrix = np.zeros((num_words, num_docs))\n",
    "\n",
    "    for doc_name, word_ids in doc_word_v.items():\n",
    "        doc_index = doc2index[doc_name]\n",
    "\n",
    "        for word_id in word_ids:\n",
    "            term_doc_matrix[word_id, doc_index] += 1\n",
    "\n",
    "    return term_doc_matrix\n",
    "\n",
    "# Example usage:\n",
    "term_doc_matrix = term_document_matrix(doc_word_vectors, doc2index, word2index)\n",
    "\n",
    "# Print the term-document matrix\n",
    "print(term_doc_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__c)__ Implement the function ``to_tf_idf_matrix`` that takes a term-document matrix and returns the corresponding term frequency (tf) matrix. If the parameter ``idf`` is set to ``True``, the tf-matrix should further be transformed to a tf-idf matrix (i.e. every entry corresponds to the tf-idf value of the associated word and document). Your implementation should leave the input term-document matrix unchanged. __(3 pts)__\n",
    "\n",
    "Recall the following formulas:\n",
    "\n",
    "\\begin{equation}\n",
    "  tf_{t,d} =\n",
    "    \\begin{cases}\n",
    "      1+log_{10}\\text{count}(t,d) & \\text{if count}(t, d) > 0\\\\\n",
    "      0 & \\text{otherwise}\n",
    "    \\end{cases}       \n",
    "\\end{equation}  \n",
    "\n",
    "$$idf_t = log_{10}(\\frac{N}{df_i})$$  \n",
    "\n",
    "$$tf-idf_{t,d} = tf_{t,d} \\cdot idf_t$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Matrix:\n",
      "[[1.         0.         1.         1.         0.         0.        ]\n",
      " [1.         1.30103    0.         1.30103    1.         0.        ]\n",
      " [1.         0.         1.         0.         0.         0.        ]\n",
      " [0.         1.         1.         0.         1.47712125 1.30103   ]\n",
      " [0.         1.         1.         0.         1.         1.30103   ]\n",
      " [0.         0.         0.         1.30103    1.         0.        ]\n",
      " [0.         0.         0.         1.         0.         1.        ]]\n",
      "\n",
      "TF-IDF Matrix:\n",
      "[[0.30103    0.         0.30103    0.30103    0.         0.        ]\n",
      " [0.17609126 0.22910001 0.         0.22910001 0.17609126 0.        ]\n",
      " [0.47712125 0.         0.47712125 0.         0.         0.        ]\n",
      " [0.         0.17609126 0.17609126 0.         0.26010814 0.22910001]\n",
      " [0.         0.17609126 0.17609126 0.         0.17609126 0.22910001]\n",
      " [0.         0.         0.         0.62074906 0.47712125 0.        ]\n",
      " [0.         0.         0.         0.47712125 0.         0.47712125]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/15/dls4kqcd379bjn30b4yp_zz00000gn/T/ipykernel_78491/1770020036.py:8: RuntimeWarning: divide by zero encountered in log10\n",
      "  tf_matrix = np.where(td_matrix > 0, 1 + np.log10(td_matrix), 0)\n"
     ]
    }
   ],
   "source": [
    "def to_tf_idf_matrix(td_matrix: NDArray[NDArray[float]], idf: bool=True) -> NDArray[NDArray[float]]:\n",
    "    \"\"\"\n",
    "    :param td_matrix: term-document matrix \n",
    "    :param idf: computes the tf-idf matrix if True, otherwise computes only the tf matrix\n",
    "    :return: matrix with tf(-idf) values for each word-document pair \n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    tf_matrix = np.where(td_matrix > 0, 1 + np.log10(td_matrix), 0)\n",
    "\n",
    "    if idf:\n",
    "        N = td_matrix.shape[1]  # Number of documents\n",
    "        df = np.count_nonzero(td_matrix, axis=1)  # Document frequency for each word\n",
    "\n",
    "        idf_matrix = np.log10(N / df)\n",
    "        tf_idf_matrix = tf_matrix * idf_matrix[:, np.newaxis]  # Broadcasting idf values to match the matrix dimensions\n",
    "\n",
    "        return tf_idf_matrix\n",
    "    else:\n",
    "        return tf_matrix\n",
    "\n",
    "# Example usage:\n",
    "tf_matrix = to_tf_idf_matrix(term_doc_matrix, idf=False)\n",
    "tf_idf_matrix = to_tf_idf_matrix(term_doc_matrix, idf=True)\n",
    "\n",
    "# Print the results\n",
    "print(\"TF Matrix:\")\n",
    "print(tf_matrix)\n",
    "\n",
    "print(\"\\nTF-IDF Matrix:\")\n",
    "print(tf_idf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__d)__ We want to test our implementation on our running example. First, print the tf-idf for each word of the query ``ice beer`` with respect to each document. Second, find the two most similar documents from ``d1, d2, d3`` according to cosine similarity and print all similarity values.  __(3 pts)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF values for the query 'ice beer' with respect to each document:\n",
      "d1: 0.408248290463863\n",
      "d2: 0.8660254037844388\n",
      "d3: 0.35355339059327373\n",
      "d4: 0.4472135954999579\n",
      "d5: 0.816496580927726\n",
      "d6: 0.4714045207910316\n",
      "\n",
      "Cosine Similarity values:\n",
      "d1 to Query: 0.9999999999999998\n",
      "d2 to Query: 0.3162277660168379\n",
      "d3 to Query: 0.9999999999999998\n",
      "\n",
      "The two most similar documents to the query:\n",
      "1. d3 - Similarity: 0.9999999999999998\n",
      "2. d1 - Similarity: 0.9999999999999998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/15/dls4kqcd379bjn30b4yp_zz00000gn/T/ipykernel_78491/1770020036.py:8: RuntimeWarning: divide by zero encountered in log10\n",
      "  tf_matrix = np.where(td_matrix > 0, 1 + np.log10(td_matrix), 0)\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List\n",
    "import numpy as np\n",
    "from numpy import ndarray as NDArray\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "word2index, doc2index, index2doc, doc_word_vectors = process_docs(docs)\n",
    "\n",
    "# Create the term-document matrix\n",
    "term_doc_matrix = term_document_matrix(doc_word_vectors, doc2index, word2index)\n",
    "\n",
    "# Convert the term-document matrix to TF-IDF matrix\n",
    "# tf_idf_matrix = to_tf_idf_matrix(term_doc_matrix, idf=True)\n",
    "\n",
    "# Query\n",
    "query = \"ice beer\"\n",
    "query_tokens = word_tokenize(query.lower())\n",
    "\n",
    "# Create a vector for the query\n",
    "query_vector = np.zeros((len(word2index),))\n",
    "for token in query_tokens:\n",
    "    if token in word2index:\n",
    "        query_vector[word2index[token]] += 1\n",
    "\n",
    "# Transform the query vector to TF-IDF\n",
    "query_vector = to_tf_idf_matrix(query_vector.reshape(1, -1), idf=True)\n",
    "\n",
    "# Print TF-IDF values for each word of the query with respect to each document\n",
    "print(\"\\nTF-IDF values for the query 'ice beer' with respect to each document:\")\n",
    "for doc_name in docs.keys():\n",
    "    doc_index = doc2index[doc_name]\n",
    "    similarity_values = cosine_similarity(query_vector, term_doc_matrix[:, doc_index].reshape(1, -1))\n",
    "\n",
    "    print(f\"{doc_name}: {similarity_values[0][0]}\")\n",
    "\n",
    "# Find the two most similar documents from d1, d2, d3 based on cosine similarity\n",
    "d1_index = doc2index[\"d1\"]\n",
    "d2_index = doc2index[\"d2\"]\n",
    "d3_index = doc2index[\"d3\"]\n",
    "\n",
    "cosine_similarities = cosine_similarity(term_doc_matrix[:, [d1_index, d2_index, d3_index]])\n",
    "most_similar_docs = np.argsort(cosine_similarities[0])[-2:][::-1]\n",
    "\n",
    "# Print all similarity values\n",
    "print(\"\\nCosine Similarity values:\")\n",
    "for i, doc_index in enumerate([d1_index, d2_index, d3_index]):\n",
    "    print(f\"d{i + 1} to Query: {cosine_similarities[0][i]}\")\n",
    "\n",
    "# Print the two most similar documents\n",
    "print(\"\\nThe two most similar documents to the query:\")\n",
    "for i, doc_index in enumerate(most_similar_docs):\n",
    "    doc_name = index2doc[doc_index]\n",
    "    print(f\"{i + 1}. {doc_name} - Similarity: {cosine_similarities[0][doc_index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__e)__ In a second step we want to find the documents that are most similar to a provided query. Therefore, implement the function ``process_query`` that creates a vector represention of the query. __(5 pts)__\n",
    "\n",
    "Create a vector that has an entry for each vocabulary word (words that appeared in any document), where the entry at position ``i`` indicates how often the word with id ``i`` (as indicated by ``word2index``) appears in the query. \n",
    "\n",
    "If ``tf`` is set to ``True``, you should transform all entries to tf-values. Similar, if ``idf`` is set to ``True``, return a vector with tf-idf values (cf. task __c)__). The computation of the idf values is based on the corresponding input term-document matrix.\n",
    "\n",
    "In case the query contains words that are in any of the documents, print an appropriate error message and stop the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Vector for Query 'ice beer': [0.         0.17609126 0.         0.17609126 0.         0.\n",
      " 0.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/15/dls4kqcd379bjn30b4yp_zz00000gn/T/ipykernel_91037/2600691652.py:19: RuntimeWarning: divide by zero encountered in log10\n",
      "  query_vector = np.where(query_vector > 0, 1 + np.log10(query_vector), 0)\n"
     ]
    }
   ],
   "source": [
    "def process_query(query: List[str], word2index: Dict[str, int], td_matrix: NDArray[NDArray[float]], tf: bool=True, idf: bool=True) -> NDArray[float]:\n",
    "    \"\"\"\n",
    "    :param query: list of query tokens\n",
    "    :param word2index: dict that maps each word to a unique id\n",
    "    :param td_matrix: term-document matrix\n",
    "    :param tf: computes the tf vector of the query if True\n",
    "    :param idf: computes the tf-idf vector of the query if True, ignored if tf=False\n",
    "    :return: vector representation of the input query (using tf(-idf))    \n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    query_vector = np.zeros((len(word2index),))\n",
    "\n",
    "    for token in query:\n",
    "        if token in word2index:\n",
    "            word_index = word2index[token]\n",
    "            query_vector[word_index] += 1\n",
    "\n",
    "    if tf:\n",
    "        query_vector = np.where(query_vector > 0, 1 + np.log10(query_vector), 0)\n",
    "\n",
    "    if idf:\n",
    "        N = td_matrix.shape[1]  # Number of documents\n",
    "        df = np.count_nonzero(td_matrix, axis=1)  # Document frequency for each word\n",
    "\n",
    "        idf_vector = np.log10(N / df)\n",
    "        query_vector *= idf_vector  # Element-wise multiplication for tf-idf values\n",
    "\n",
    "    return query_vector\n",
    "\n",
    "# Example usage:\n",
    "query = [\"ice\", \"beer\"]\n",
    "query_vector_tf_idf = process_query(query, word2index, term_doc_matrix, tf=True, idf=True)\n",
    "print(\"TF-IDF Vector for Query 'ice beer':\", query_vector_tf_idf)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__f)__ Implement a function ``most_similar_docs`` that gets the vector representation of a query (in terms of counts, tf values or tf-idf values) and a term-document matrix that can either contain counts, tf-values or tf-idf values.  Compute the cosine similarity between the query and all documents and return the document names and the cosine similarity values of the top-``k`` documents that are most similar to the query. The value of ``k`` should be specified by the user. __(3 pts)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 2 most similar documents to the query:\n",
      "1. d2 - Similarity: 0.8660254037844388\n",
      "2. d5 - Similarity: 0.8164965809277261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/15/dls4kqcd379bjn30b4yp_zz00000gn/T/ipykernel_91037/2600691652.py:19: RuntimeWarning: divide by zero encountered in log10\n",
      "  query_vector = np.where(query_vector > 0, 1 + np.log10(query_vector), 0)\n"
     ]
    }
   ],
   "source": [
    "def most_similar_docs(query_v: NDArray[float], td_matrix: NDArray[NDArray[float]], index2doc: Dict[int, str], k: int) -> (List[str], List[float]):\n",
    "    \"\"\"\n",
    "    :param query_v: vector representation of the input query\n",
    "    :param td_matrix: term-document matrix, possibly with tf-(idf) values \n",
    "    :param index2doc: dict that maps each document id to its name\n",
    "    :k: number of documents to return\n",
    "    :returns:\n",
    "        - list with names of the top-k most similar documents to the query, ordered by descending similarity\n",
    "        - list with cosine similarities of the top-k most similar docs, ordered by descending similarity\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    cosine_similarities = cosine_similarity(query_v.reshape(1, -1), td_matrix.T)\n",
    "\n",
    "    # Get the indices of the top-k most similar documents\n",
    "    top_k_indices = np.argsort(cosine_similarities[0])[-k:][::-1]\n",
    "\n",
    "    # Get the document names and similarity values\n",
    "    top_k_docs = [index2doc[i] for i in top_k_indices]\n",
    "    top_k_similarities = [cosine_similarities[0, i] for i in top_k_indices]\n",
    "\n",
    "    return top_k_docs, top_k_similarities\n",
    "\n",
    "# Example usage:\n",
    "query_vector_tf_idf = process_query([\"ice\", \"beer\"], word2index, term_doc_matrix, tf=True, idf=True)\n",
    "top_k_docs, top_k_similarities = most_similar_docs(query_vector_tf_idf, term_doc_matrix, index2doc, k=2)\n",
    "\n",
    "# Print the results\n",
    "print(f\"\\nTop 2 most similar documents to the query:\")\n",
    "for i, doc in enumerate(top_k_docs):\n",
    "    print(f\"{i + 1}. {doc} - Similarity: {top_k_similarities[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Text Classification (17pts)\n",
    "In this task, we want to build a logistic regression classifier to classify 20newsgroups posts. As feature representation, we want to use tf-idf vectors as just implemented.\n",
    "\n",
    "### Logistic Regression\n",
    "Implement a logistic regression classifier, similar to exercise 7. Again, you don't need to add a bias weight/feature.\n",
    "\n",
    "__a)__ Implement the `predict_proba` function in the `LogisticRegression` class below. Your function should return the output of a logistic regression classifier according to the current assignments of weights $\\mathbf{w}$, i.e., \n",
    "$$\n",
    "expit(\\mathbf{w}^T\\mathbf{x})\n",
    "$$\n",
    "You can assume that model weights are stored in a variable `self.w`. __(3pts)__\n",
    "\n",
    "__b)__ Implement the `predict` function in the `LogisticRegression` class below. The prediction should return class `1` if the classifier output is above 0.5, otherwise `0` __(3pts)__\n",
    "\n",
    "__c)__ Implement the `fit` function to learn the model parameters `w` with stochastic gradient descent for one epoch, i.e., going over the training data once. Store the learned parameters in a variable `self.w`. Only initialize the parameters randomly in the first training iteration and continue with learned parameters in later iterations. Make sure, that you iterate over instances in each epoch randomly.  __(5pts)__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "\n",
    "class LogisticRegression():\n",
    "    '''Logistic Regression Classifier.'''\n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "    \n",
    "    def fit(self, x: NDArray[NDArray[float]], y: NDArray[int], eta: float=0.1):\n",
    "        '''\n",
    "        :param x: 2D numpy array where each row is an instance\n",
    "        :param y: 1D numpy array with target classes for instances in x\n",
    "        :param eta: learning rate, default is 0.1\n",
    "        :param epochs: fixed number of epochs as stopping criterion, default is 10\n",
    "        '''\n",
    "        # c)\n",
    "        # Initialize weights randomly if not already initialized\n",
    "        if self.w is None:\n",
    "            self.w = np.random.randn(x.shape[1])\n",
    "        \n",
    "        # Shuffle the data\n",
    "        indices = np.arange(x.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        x = x[indices]\n",
    "        y = y[indices]\n",
    "        \n",
    "        for i in range(x.shape[0]):\n",
    "            xi = x[i]\n",
    "            yi = y[i]\n",
    "            \n",
    "            # Predict\n",
    "            y_pred = self.predict_proba(xi)\n",
    "            \n",
    "            # Update weights\n",
    "            gradient = xi * (y_pred - yi)\n",
    "            self.w -= eta * gradient\n",
    "        \n",
    "    def predict_proba(self, x):\n",
    "        # a)\n",
    "        return expit(np.dot(x, self.w))\n",
    "        \n",
    "    def predict(self, x):\n",
    "        # b)\n",
    "        return (self.predict_proba(x) > 0.5).astype(int)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__e)__ Apply your model to the two categories 'comp.windows.x' and 'rec.motorcycles' from the 20newsgroups data. To this end, first transform the training data to tf-idf representation with the functions `process_docs`, `term_document_matrix` and `to_tfidf_matrix`. Transform the test documents with `process_query`. Fit your model on the training data for 10 epochs. Calculate the accuracy on the test data. __(6pts)__\n",
    "\n",
    "**Shortcut**: use the `TfidfVectorizer` from scikit learn (you may need to transform its output to a dense (array) representation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import math\n",
    "import re\n",
    "\n",
    "train = fetch_20newsgroups(subset='train', categories=['comp.windows.x','rec.motorcycles'])\n",
    "test = fetch_20newsgroups(subset='test', categories=['comp.windows.x','rec.motorcycles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
