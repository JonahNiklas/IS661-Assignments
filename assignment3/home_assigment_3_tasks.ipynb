{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "080be2e1",
   "metadata": {},
   "source": [
    "Text Analytics I HWS 23/24\n",
    "\n",
    "# Home Assignment 3 (30pts)\n",
    "\n",
    "Submit your solution via Ilias until 23.59h on Thursday, November 9th. Late submissions are **not possible**.\n",
    "\n",
    "Submit your solutions in teams of 3-4 students. Unless explicitly agreed otherwise in advance, **submissions from teams with more or less members will NOT be graded**.\n",
    "List all members of the team with their student ID in the cell below, and submit only one notebook per team. Only submit a notebook, do not submit the dataset(s) you used. Also, do NOT compress/zip your submission!\n",
    "\n",
    "You may use the code from the exercises and basic functionalities that are explained in official documentation of Python packages without citing, __all other sources must be cited__. In case of plagiarism (copying solutions from other teams or from the internet) ALL team members may be expelled from the course without warning.\n",
    "\n",
    "#### General guidelines:\n",
    "* Make sure that your code is executable, any task for which the code does not directly run on our machine will be graded with 0 points.\n",
    "* If you use packages that are not available on the default or conda-forge channel, list them below. Also add a link to installation instructions. \n",
    "* Ensure that the notebook does not rely on the current notebook or system state!\n",
    "  * Use `Kernel --> Restart & Run All` to see if you are using any definitions, variables etc. that \n",
    "    are not in scope anymore.\n",
    "  * Do not rename any of the datasets you use, and load it from the same directory that your ipynb-notebook is located in, i.e., your working directory.\n",
    "* Make sure you clean up your code before submission, e.g., properly align your code, and delete every line of code that you do not need anymore, even if you may have experimented with it. Minimize usage of global variables. Avoid reusing variable names multiple times!\n",
    "* Ensure your code/notebook terminates in reasonable time.\n",
    "* Feel free to use comments in the code. While we do not require them to get full marks, they may help us in case your code has minor errors.\n",
    "* For questions that require a textual answer, please do not write the answer as a comment in a code cell, but in a Markdown cell below the code. Always remember to provide sufficient justification for all answers.\n",
    "* You may create as many additional cells as you want, just make sure that the solutions to the individual tasks can be found near the corresponding assignment.\n",
    "* If you have any general question regarding the understanding of some task, do not hesitate to post in the student forum in Ilias, so we can clear up such questions for all students in the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "275631a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# studentIDs of all team members\n",
    "team_members = [12345,67899,880800,234242]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ed9164",
   "metadata": {},
   "source": [
    "Additional packages (if any):\n",
    " - Example: `powerlaw`, https://github.com/jeffalstott/powerlaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36ef36f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union, Dict, Set, Tuple\n",
    "from numpy.typing import NDArray\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acc6b92",
   "metadata": {},
   "source": [
    "### Task 1: POS tagging (6 points)\n",
    "\n",
    "In this task, we want to explore sentences with similar part of speech (POS) tag structure. For this, we need a corpus of text with tags. We will generate such a corpus by using NLTKâ€™s currently recommended POS tagger to tag a given list of tokens (https://www.nltk.org/api/nltk.tag.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01d59e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK's off-the-shelf POS tagger\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e78bb0d",
   "metadata": {},
   "source": [
    "__a)__ Given a corpus of text ``corpus`` as a sequence of tokens, we want to collect all words that are tagged with a certain POS tag. Implement a function ``collect_words_for_tag`` that first tags the given corpus using NLTK's off-the-shelf tagger imported in the cell above. Then, for each POS tag, collect all words that were tagged with it. You should return a dictionary that maps each POS tag that was observed to the set of words that were assigned this tag in the given corpus. __(2 pts)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5651149b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/yinhsuan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag as nltk_pos_tag\n",
    "from nltk.corpus.reader.util import StreamBackedCorpusView \n",
    "from typing import List, Dict, Set, Union\n",
    "import nltk\n",
    "\n",
    "# Download the NLTK data files for part-of-speech tagging\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def collect_words_for_tag(corpus: Union[List[str], StreamBackedCorpusView]) -> Dict[str, Set[str]]:\n",
    "    '''\n",
    "    :param corpus: sequence of tokens that represents the text corpus\n",
    "    :return: dict that maps each tag to a set of tokens that were assigned this tag in the corpus\n",
    "    '''\n",
    "    # Initialize an empty list to store the tagged tokens\n",
    "    tagged_corpus = []\n",
    "\n",
    "    # Iterate through the sentences in the corpus and tag their tokens\n",
    "    for sentence in corpus:\n",
    "        tagged_sentence = nltk_pos_tag(sentence)\n",
    "        tagged_corpus.extend(tagged_sentence)\n",
    "\n",
    "    # Create a dictionary to store words for each POS tag\n",
    "    words_by_tag = {}\n",
    "\n",
    "    # Iterate through the tagged tokens and collect words for each tag\n",
    "    for token, pos_tag in tagged_corpus:\n",
    "        if pos_tag not in words_by_tag:\n",
    "            words_by_tag[pos_tag] = set()\n",
    "        words_by_tag[pos_tag].add(token)\n",
    "\n",
    "    return words_by_tag\n",
    "\n",
    "\n",
    "# # Sample corpus\n",
    "# sample_corpus = [\"This\", \"is\", \"a\", \"sample\", \"sentence\", \"for\", \"testing\", \"POS\", \"tagging\", \".\"]\n",
    "\n",
    "# # Test the function with the sample corpus\n",
    "# result = collect_words_for_tag(sample_corpus)\n",
    "\n",
    "# # Print the results\n",
    "# for pos_tag, words in result.items():\n",
    "#     print(f\"POS Tag: {pos_tag}\")\n",
    "#     print(f\"Words: {words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba737321",
   "metadata": {},
   "source": [
    "__b)__ Implement a function ``generate_sentences`` that gets a sentence and a POS dictionary (assume the POS dictionary was generated by your function in __a)__) as input and generates ``n`` sequences of words with the same tag structure. The words in your generated sequence should be randomly taken from the set of words associated with the current tag. \n",
    "\n",
    "Additionally, the user should have the option to achieve sentences of ``better_quality``. Thus, if ``better_quality=True``, make sure that the tag structure of the output sentences actually matches the tag structure of the input sentence, as the tags may change depending on the context. \n",
    "\n",
    "You can assume that the training corpus is large enough to include all possible POS tags. __(2 pts)__\n",
    "\n",
    "_Hint: consider the_ ``random`` _module_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b4efad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: This/DT is/VBZ a/DT sample/NN\n",
      "Sentence 2: This/DT is/VBZ a/DT sample/NN\n",
      "Sentence 3: This/DT is/VBZ a/DT sample/NN\n",
      "Sentence 4: This/DT is/VBZ a/DT sample/NN\n",
      "Sentence 5: This/DT is/VBZ a/DT sample/NN\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from typing import List, Dict, Set\n",
    "from nltk import pos_tag\n",
    "\n",
    "def generate_rand(sentence: List[str], pos_dict: Dict[str, Set[str]], n: int, better_quality: bool=False) -> List[List[str]]:\n",
    "    generated_sentences = []\n",
    "    input_tags = []  # To store the POS tags of the input sentence.\n",
    "\n",
    "    # Tokenize the input sentence into words and POS tags.\n",
    "    for word in sentence:\n",
    "        for pos_tag, word_set in pos_dict.items():\n",
    "            if word in word_set:\n",
    "                input_tags.append(pos_tag)\n",
    "                break\n",
    "\n",
    "    if len(input_tags) == 0:\n",
    "        # If any word in the input sentence doesn't have a corresponding POS tag in the pos_dict, return an empty list.\n",
    "        return []\n",
    "\n",
    "    for _ in range(n):\n",
    "        generated_sentence = []\n",
    "\n",
    "        for input_word, input_tag in zip(sentence, input_tags):\n",
    "            if better_quality:\n",
    "                # If better_quality is True, select a random word from the set associated with the input POS tag.\n",
    "                generated_word = random.choice(list(pos_dict[input_tag]))\n",
    "            else:\n",
    "                # If better_quality is False, select a random word from any set in the pos_dict.\n",
    "                generated_word = random.choice(list(random.choice(list(pos_dict.values()))))\n",
    "\n",
    "            generated_sentence.append(generated_word)\n",
    "\n",
    "        generated_sentences.append(generated_sentence)\n",
    "\n",
    "    return generated_sentences\n",
    "\n",
    "# Sample input sentence with POS tags\n",
    "input_sentence = [\n",
    "    (\"This\", \"DT\"),\n",
    "    (\"is\", \"VBZ\"),\n",
    "    (\"a\", \"DT\"),\n",
    "    (\"sample\", \"NN\"),\n",
    "    (\"sentence\", \"NN\"),\n",
    "    (\"for\", \"IN\"),\n",
    "    (\"testing\", \"VBG\"),\n",
    "    (\"POS\", \"NNS\"),\n",
    "    (\"tagging\", \"NN\"),\n",
    "    (\".\", \".\")\n",
    "]\n",
    "\n",
    "# Create a sample POS dictionary\n",
    "pos_dict = {\n",
    "    \"DT\": {\"the\", \"this\", \"a\"},\n",
    "    \"VBZ\": {\"is\", \"am\", \"are\"},\n",
    "    \"NN\": {\"cat\", \"dog\", \"book\"},\n",
    "    \"IN\": {\"for\", \"on\", \"with\"},\n",
    "    \"VBG\": {\"running\", \"playing\", \"working\"},\n",
    "    \"NNS\": {\"cats\", \"dogs\", \"books\"},\n",
    "    \".\": {\".\", \"!\", \"?\"}\n",
    "}\n",
    "\n",
    "# Extract words from the input sentence\n",
    "input_words = [word for word, _ in input_sentence]\n",
    "\n",
    "# Generate 5 random sentences with the same tag structure as the input sentence\n",
    "generated_sentences = generate_rand(input_words, pos_dict, n=5, better_quality=False)\n",
    "\n",
    "# Print the generated sentences\n",
    "for i, sentence in enumerate(generated_sentences, start=1):\n",
    "    sentence_str = ' '.join([word + \"/\" + tag for word, (word, tag) in zip(sentence, input_sentence)])\n",
    "    print(f\"Sentence {i}: {sentence_str}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a9b3ba",
   "metadata": {},
   "source": [
    "__c)__ Using the input sentence ``This test is very difficult``, test your implementation to generate 10 sentences based on  \n",
    "\n",
    "* \"Emma\" by Jane Austen\n",
    "\n",
    "* The \"King James Bible\"\n",
    "\n",
    "Store your POS dictionary in ``emma_tags``and ``bible_tags``, respectively. Your generated sentences should be stored in ``emma_sent`` and ``bible_sent``. __(2 pts)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a69ab118",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = [\"This\", \"test\", \"is\", \"very\", \"difficult\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad042eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/yinhsuan/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['what', 'sooner', 'surprize', 'endeavour'], ['indisputable', 'aloud', 'farther', ')'], ['tottering', 'owned', 'across', 'Do'], [';', 'up', 'how', 'speedy'], ['weather', 'ashamed', 'weather', 'attend'], ['gratification', 'understood', '\"', 'flatterer'], ['amid', 'Such', '(', 'Coles'], ['s', 'mis', 'fifty', 'Prince'], ['softer', '(', '?', '('], ['acquire', '.', 'suppose', 'longer']]\n",
      "[['wherein', 'thee', 'To'], ['speaketh', 'ye', 'set'], [',', 'their', 'reasons'], ['dipped', 'Oh', 'no'], [';)', 'must', 'most'], ['.', 'therein', 'Jasiel'], ['mourneth', ')', ')'], ['!', 'riot', ','], ['manna', 'denied', '('], ['killest', 'behind', 'kin']]\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "# Download the \"Emma\" and \"King James Bible\" corpora\n",
    "nltk.download('gutenberg')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# Load the \"Emma\" and \"King James Bible\" corpora\n",
    "# emma_corpus = [sent for sent in nltk.corpus.gutenberg.sents('austen-emma.txt') if not any(word.isdigit() for word in sent)]\n",
    "# bible_corpus = [sent for sent in nltk.corpus.gutenberg.sents('bible-kjv.txt') if not any(word.isdigit() for word in sent)]\n",
    "emma_corpus = gutenberg.sents('austen-emma.txt')\n",
    "bible_corpus = gutenberg.sents('bible-kjv.txt')\n",
    "\n",
    "# Prepare POS dictionaries for \"Emma\" and \"King James Bible\"\n",
    "emma_tags = collect_words_for_tag(emma_corpus)\n",
    "bible_tags = collect_words_for_tag(bible_corpus)\n",
    "\n",
    "# Generate sentences based on \"Emma\" corpus\n",
    "emma_sent = generate_rand(sent, emma_tags, n=10, better_quality=False)\n",
    "print(emma_sent)\n",
    "\n",
    "# Generate sentences based on \"King James Bible\" corpus\n",
    "bible_sent = generate_rand(sent, bible_tags, n=10, better_quality=False)\n",
    "print(bible_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beecad4e",
   "metadata": {},
   "source": [
    "### Task 2: The Viterbi algorithm (12 points)\n",
    "Implement the Viterbi algorithm as introduced in the lecture (lecture 8, slide 20) and the exercise. The input of your function is a sentence that should be tagged, a dictionary with state transition probabilites and a dictionary with word emission probabilities. You may assume that the _transition probabilities_ are complete, i.e. the dictionary includes every combination of states. In contrast, we assume that all combinations of words and POS tags that are not in the dictionary of _emission probabilities_ have an emission probability of 0.\n",
    "\n",
    "The function should return a list of POS tags, s.t. that each tag corresponds to a word of the input sentence. Moreover, return the probability of the sequence of POS tags that you found. \n",
    "\n",
    "You can test your function on the given example that was discussed in the Pen&Paper exercise. For the sentence ``the fans watch the race`` and the provided probabilities, your function should return the POS tag sequence ``['DT', 'N', 'V', 'DT', 'N']`` and a probability of ``9.720000000000002e-06``.\n",
    "\n",
    "Additionally, implement beam search in the viterbi algorithm. The beam size is defined by the parameter `beam`. For example for `beam=2` we only keep the best 2 scores per column in each step and discard the rest. You may use the example from the lecture to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8319309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test sentence\n",
    "sentence = [\"the\", \"fans\", \"watch\", \"the\", \"race\"]\n",
    "\n",
    "# state transition probabilities (complete)\n",
    "state_trans_prob = {('<s>','DT'):0.8,('<s>','N'):0.2,('<s>','V'):0.0,\n",
    "                    ('DT','DT'):0.0,('DT','N'):0.9,('DT','V'):0.1,\n",
    "                    ('N','DT'):0.0,('N','N'):0.5,('N','V'):0.5,\n",
    "                    ('V','DT'):0.5,('V','N'):0.5,('V','V'):0.0}\n",
    "\n",
    "# word emission probabilities (not complete, all combinations that are not present have probability 0)\n",
    "word_emission_prob = {('the','DT'):0.2, ('fans','N'):0.1,('fans','V'):0.2,('watch','N'):0.3,\n",
    "                      ('watch','V'):0.15,('race','N'):0.1,('race','V'):0.3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa9e4d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viterbi without beam search: (['DT', 'N', 'V', 'DT', 'N'], 1.215e-05)\n",
      "Viterbi with beam search: (['V', 'V', 'V', 'V', 'V'], 0.2)\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict, Tuple\n",
    "\n",
    "def Viterbi(sentence: List[str], trans_prob: Dict[Tuple[str,str], float], emiss_prob: Dict[Tuple[str,str], float], beam: int=0) -> (List[str], float):\n",
    "    '''\n",
    "    :param sentence: sentence that we want to tag\n",
    "    :param trans_prob: dict with state transition probabilities\n",
    "    :param emiss_prob: dict with word emission probabilities\n",
    "    :param beam: beam size for beam search. If 0, don't apply beam search\n",
    "    :returns: \n",
    "        - list with POS tags for each input word\n",
    "        - float that indicates the probability of the tag sequence\n",
    "    '''\n",
    "    # Initialize matrices to store probabilities and backpointers\n",
    "    n = len(sentence)\n",
    "    states = list(set(state for state, _ in trans_prob.keys()))\n",
    "    num_states = len(states)\n",
    "    probabilities = [[0.0] * num_states for _ in range(n)]\n",
    "    backpointers = [[-1] * num_states for _ in range(n)]\n",
    "\n",
    "    # Initialize the first column with initial probabilities\n",
    "    for j, state in enumerate(states):\n",
    "        word_state = (sentence[0], state)\n",
    "        if word_state in emiss_prob:\n",
    "            probabilities[0][j] = emiss_prob[word_state]\n",
    "        else:\n",
    "            probabilities[0][j] = 0.0\n",
    "\n",
    "    # Viterbi algorithm\n",
    "    for i in range(1, n):\n",
    "        for j, state in enumerate(states):\n",
    "            max_prob = 0.0\n",
    "            best_prev_state = None\n",
    "\n",
    "            for k, prev_state in enumerate(states):\n",
    "                transition_prob = trans_prob.get((prev_state, state), 0.0)\n",
    "                word_state = (sentence[i], state)\n",
    "                emiss_prob_value = emiss_prob.get(word_state, 0.0)\n",
    "\n",
    "                prev_prob = probabilities[i - 1][k]\n",
    "                current_prob = prev_prob * transition_prob * emiss_prob_value\n",
    "\n",
    "                if current_prob > max_prob:\n",
    "                    max_prob = current_prob\n",
    "                    best_prev_state = k\n",
    "\n",
    "            probabilities[i][j] = max_prob\n",
    "            backpointers[i][j] = best_prev_state\n",
    "\n",
    "    # Trace back to find the best path\n",
    "    best_path = [0] * n\n",
    "    max_prob = max(probabilities[-1])\n",
    "    best_path[-1] = probabilities[-1].index(max_prob)\n",
    "    for i in range(n - 2, -1, -1):\n",
    "        best_path[i] = backpointers[i + 1][best_path[i + 1]]\n",
    "\n",
    "    best_path_tags = [states[state_index] for state_index in best_path]\n",
    "\n",
    "    # Calculate the probability of the best path\n",
    "    probability = max_prob\n",
    "\n",
    "    if beam == 0:\n",
    "        return best_path_tags, probability\n",
    "    else:\n",
    "        return apply_beam_search(probabilities, backpointers, states, beam)\n",
    "\n",
    "def apply_beam_search(probabilities, backpointers, states, beam):\n",
    "    n = len(probabilities)\n",
    "    num_states = len(states)\n",
    "    best_path = [0] * n\n",
    "    max_probs = [0.0] * num_states\n",
    "\n",
    "    for i in range(n - 1, -1, -1):\n",
    "        sorted_states = sorted(range(num_states), key=lambda j: -probabilities[i][j])\n",
    "        for j in range(min(beam, num_states)):\n",
    "            state = sorted_states[j]\n",
    "            best_path[i] = state\n",
    "            max_probs[state] = probabilities[i][state]\n",
    "\n",
    "        # Update backpointers for the next column\n",
    "        if i > 0:\n",
    "            for j in range(num_states):\n",
    "                backpointers[i - 1][j] = best_path[i]\n",
    "\n",
    "    best_path_tags = [states[state_index] for state_index in best_path]\n",
    "    probability = max(max_probs)\n",
    "\n",
    "    return best_path_tags, probability\n",
    "\n",
    "# Without beam search\n",
    "result_no_beam = Viterbi(sentence, state_trans_prob, word_emission_prob)\n",
    "print(\"Viterbi without beam search:\", result_no_beam)\n",
    "\n",
    "# With beam search (beam size = 2)\n",
    "result_with_beam = Viterbi(sentence, state_trans_prob, word_emission_prob, beam=2)\n",
    "print(\"Viterbi with beam search:\", result_with_beam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c38fe7",
   "metadata": {},
   "source": [
    "### Task 3: ML Basics - Naive Bayes Classification (10pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3ab66f",
   "metadata": {},
   "source": [
    "### Task 3: ML Basics - Naive Bayes Classification (12pts)\n",
    "In this task, we want to build a Naive Bayes classifier with add-1 smoothing for text classification (pseudocode given below), e.g., to assign a category to a document. Use the class-skeleton provided below for your implementation.\n",
    "\n",
    "#### Naive Bayes Pseudocode\n",
    "##### TrainMultiNomialNB($\\mathbb C$,$\\mathbb D$)  \n",
    "$V \\leftarrow extractVocabulary(\\mathbb D)$  \n",
    "$N \\leftarrow countDocs(\\mathbb D)$    \n",
    "for $c \\in \\mathbb C$:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$N_c \\leftarrow countDocsInClass(\\mathbb D, c)$  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$prior[c] \\leftarrow \\frac{N_c}{N}$  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$text_c \\leftarrow concatenateTextOfAllDocsInClass(\\mathbb D, c)$   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;for $t \\in V$:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$T_{ct} \\leftarrow countTokensOfTerm(text_c,t)$  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;for $t \\in V$:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$condprob[t][c] \\leftarrow \\frac{T_{ct} + 1}{\\sum_{t'}(T_{ct'} + 1)}$  \n",
    "return $V,prior,condprob$\n",
    "\n",
    "##### ApplyMultinomialNB($\\mathbb C,V,prior,condprob,d$)\n",
    "$W \\leftarrow extractTokensFromDoc(V,d)$   \n",
    "for $c \\in \\mathbb C$:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$score[c] \\leftarrow log(prior[c])$  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;for $t \\in W$:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$score[c] += log(condprob[t][c])$  \n",
    "return $argmax_{c \\in \\mathbb C} score[c]$\n",
    "\n",
    "__a) Tokenization (1pt)__  \n",
    "Implement the function `tokenize` to transform a text document to a list of tokens with the regex pattern `\\b\\w\\w+\\b`. Transform all tokens to lowercase.\n",
    "\n",
    "__b) Naive Bayes \"Training\" (6pts)__  \n",
    "Implement the `__init__` function to set up the Naive Bayes Model. Cf. TrainMultiNomialNB($\\mathbb C$,$\\mathbb D$) in the pseudocode above. Contrary to the pseudocode, the `__init__` function should not return anything, but the vocabulary, priors and conditionals should be stored in class variables. We only want to keep tokens with a frequeny > `min_count` in the vocabulary.\n",
    "\n",
    "__c) Naive Bayes Classification (3pts)__  \n",
    "Implement the `classify` function to return the most probable class for the provided document according to your Naive Bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f9d9056",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "    '''Naive Bayes for text classification.'''\n",
    "    def __init__(self, docs: List[str], labels: List[int], min_count: int=1):\n",
    "        '''\n",
    "        :param docs: list of documents from which to build the model (corpus)\n",
    "        :param labels: list of classes assigned to the list of documents (labels[i] is the class for docs[i])\n",
    "        :param min_count: minimum frequency of token in vocabulary (tokens that occur less times are discarded)\n",
    "        '''\n",
    "        # your code for Task 3b) here\n",
    "                \n",
    "    def tokenize(self, doc: str):\n",
    "        '''\n",
    "        :param doc: document to tokenize\n",
    "        :return: document as a list of tokens\n",
    "        '''\n",
    "        # your code for Task 3a) here\n",
    "\n",
    "    def classify(self, doc: str):\n",
    "        '''\n",
    "        :param doc: document to classify\n",
    "        :return: most probable class\n",
    "        '''\n",
    "        # your code for Task 3c) here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bca1195",
   "metadata": {},
   "source": [
    "__d) Evaluation (2pts)__\n",
    "Test your implementation on the 20newsgroups dataset. If implemented correctly, with `min_count=1` your Naive Bayes classifier should obtain the same accuracy as the implementation by scikit-learn (see below for comparison)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aea57aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcb0288b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# see https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html for details\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "train = fetch_20newsgroups(subset='train')\n",
    "test = fetch_20newsgroups(subset='test')\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "x = vectorizer.fit_transform(train.data)\n",
    "clf = MultinomialNB()\n",
    "clf.fit(x,train.target)\n",
    "\n",
    "pred = clf.predict(vectorizer.transform(test.data))\n",
    "\n",
    "accuracy_score(test.target,pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
